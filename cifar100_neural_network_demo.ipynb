{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000,)\n"
     ]
    }
   ],
   "source": [
    "file_name = \"data_batch_\"\n",
    "path = \"/Users/zetong/cifar-10-batches-py/\"\n",
    "X = []\n",
    "y = []\n",
    "batch_size = 16\n",
    "for i in range(1, 6):\n",
    "    name = file_name + str(i)\n",
    "    file_path = path + name\n",
    "    dic = unpickle(file_path)\n",
    "    X.append(dic[b'data'])\n",
    "    y.append(dic[b'labels'])\n",
    "X = np.concatenate(X)\n",
    "y = np.concatenate(y)\n",
    "X = X.reshape((X.shape[0], 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "def normalize(x):\n",
    "    min_val = np.min(x)\n",
    "    max_val = np.max(x)\n",
    "    x = (x-min_val) / (max_val-min_val)\n",
    "    return x\n",
    "X = normalize(X)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29750, 32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "train = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size).repeat()\n",
    "test = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size).repeat()\n",
    "val = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0908 01:47:35.190494 4661999040 deprecation.py:506] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1048704   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,050,890\n",
      "Trainable params: 1,050,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "i = tf.keras.layers.Input(shape=(32,32,3))\n",
    "conv = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(i)\n",
    "conv = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(conv)\n",
    "max_pool = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv)\n",
    "flat = tf.keras.layers.Flatten()(max_pool)\n",
    "dense = tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_uniform')(flat)\n",
    "out = tf.keras.layers.Dense(10, activation='softmax')(dense)\n",
    "model = tf.keras.Model(i, out)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0908 01:47:51.828736 4661999040 training_utils.py:1300] Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2000/2000 [==============================] - 49s 25ms/step - loss: 1.5008 - acc: 0.4683 - val_loss: 1.2711 - val_acc: 0.5402\n",
      "Epoch 2/100\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.1755 - acc: 0.5909 - val_loss: 1.2064 - val_acc: 0.5714\n",
      "Epoch 3/100\n",
      "2000/2000 [==============================] - 17s 9ms/step - loss: 1.0460 - acc: 0.6359 - val_loss: 1.1600 - val_acc: 0.5960\n",
      "Epoch 4/100\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.9408 - acc: 0.6760 - val_loss: 1.1970 - val_acc: 0.5865\n",
      "Epoch 5/100\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.8507 - acc: 0.7076 - val_loss: 1.2259 - val_acc: 0.5874\n",
      "Epoch 6/100\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.7735 - acc: 0.7344 - val_loss: 1.2301 - val_acc: 0.5912\n",
      "Epoch 7/100\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.6995 - acc: 0.7607 - val_loss: 1.2713 - val_acc: 0.5990\n",
      "Epoch 8/100\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.6322 - acc: 0.7847 - val_loss: 1.3490 - val_acc: 0.6008\n",
      "Epoch 9/100\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.5643 - acc: 0.8118 - val_loss: 1.6552 - val_acc: 0.5579\n",
      "Epoch 10/100\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.5076 - acc: 0.8312 - val_loss: 1.5273 - val_acc: 0.5829\n",
      "Epoch 11/100\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.4623 - acc: 0.8474 - val_loss: 1.7549 - val_acc: 0.5613\n",
      "Epoch 12/100\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.4285 - acc: 0.8551 - val_loss: 1.7829 - val_acc: 0.5830\n",
      "Epoch 13/100\n",
      "2000/2000 [==============================] - 17s 8ms/step - loss: 0.3910 - acc: 0.8665 - val_loss: 2.0364 - val_acc: 0.5503\n",
      "Epoch 14/100\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.3536 - acc: 0.8805 - val_loss: 2.1789 - val_acc: 0.5554\n",
      "Epoch 15/100\n",
      "2000/2000 [==============================] - 17s 9ms/step - loss: 0.3255 - acc: 0.8861 - val_loss: 2.4479 - val_acc: 0.5421\n",
      "Epoch 16/100\n",
      "2000/2000 [==============================] - 17s 8ms/step - loss: 0.3009 - acc: 0.8967 - val_loss: 2.3108 - val_acc: 0.5495\n",
      "Epoch 17/100\n",
      "2000/2000 [==============================] - 17s 8ms/step - loss: 0.2763 - acc: 0.9037 - val_loss: 2.6273 - val_acc: 0.5438\n",
      "Epoch 18/100\n",
      "2000/2000 [==============================] - 17s 8ms/step - loss: 0.2515 - acc: 0.9113 - val_loss: 2.7223 - val_acc: 0.5383\n",
      "Epoch 19/100\n",
      "2000/2000 [==============================] - 17s 8ms/step - loss: 0.2327 - acc: 0.9202 - val_loss: 2.6390 - val_acc: 0.5644\n",
      "Epoch 20/100\n",
      "2000/2000 [==============================] - 17s 9ms/step - loss: 0.2210 - acc: 0.9243 - val_loss: 2.9090 - val_acc: 0.5366\n",
      "Epoch 21/100\n",
      "2000/2000 [==============================] - 17s 9ms/step - loss: 0.2027 - acc: 0.9285 - val_loss: 2.8549 - val_acc: 0.5570\n",
      "Epoch 22/100\n",
      "2000/2000 [==============================] - 17s 9ms/step - loss: 0.1909 - acc: 0.9331 - val_loss: 3.1906 - val_acc: 0.5509\n",
      "Epoch 23/100\n",
      "2000/2000 [==============================] - 17s 9ms/step - loss: 0.1866 - acc: 0.9335 - val_loss: 3.0295 - val_acc: 0.5480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x116b9c390>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopping_early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=20)\n",
    "model.fit(train, callbacks=[stopping_early], validation_data=val, validation_steps=X_val.shape[0] / batch_size, steps_per_epoch=2000, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/937 [==============================] - 15s 16ms/step - loss: 2.9624 - acc: 0.5611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.963961192639669, 0.5610667]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test, steps=X_test.shape[0] / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "X = X.reshape((X.shape[0], 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

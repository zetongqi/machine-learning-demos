{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from numpy import linalg as LA\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sparse_autoencoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(sparse_autoencoder, self).__init__()\n",
    "        input_size = 784\n",
    "        output_size = 784      \n",
    "        self.hidden = nn.Linear(input_size, 256)\n",
    "        self.out = nn.Linear(256, output_size)\n",
    "        self.batch_size = 8\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.sigmoid(self.hidden(x))\n",
    "        y_hat = self.out(h)\n",
    "        return y_hat, h\n",
    "    \n",
    "    def loss(self, x, y, beta=0.2, rho=0.1):\n",
    "        y_hat, h = self.forward(x)\n",
    "        rho_hat = torch.sum(h, dim=0) / self.batch_size\n",
    "        c = nn.MSELoss()\n",
    "        l = c(y_hat, y) + beta * torch.sum(rho * torch.log(rho / rho_hat) + (1-rho) * torch.log((1-rho) / (1-rho_hat)))\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = sparse_autoencoder()\n",
    "optimizer = optim.Adam(ae.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train\n",
    "trans = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1667, grad_fn=<DivBackward0>)\n",
      "tensor(0.1815, grad_fn=<DivBackward0>)\n",
      "tensor(0.1628, grad_fn=<DivBackward0>)\n",
      "tensor(0.1576, grad_fn=<DivBackward0>)\n",
      "tensor(0.1630, grad_fn=<DivBackward0>)\n",
      "tensor(0.1495, grad_fn=<DivBackward0>)\n",
      "tensor(0.1409, grad_fn=<DivBackward0>)\n",
      "tensor(0.1453, grad_fn=<DivBackward0>)\n",
      "tensor(0.1351, grad_fn=<DivBackward0>)\n",
      "tensor(0.1488, grad_fn=<DivBackward0>)\n",
      "tensor(0.1334, grad_fn=<DivBackward0>)\n",
      "tensor(0.1270, grad_fn=<DivBackward0>)\n",
      "tensor(0.1234, grad_fn=<DivBackward0>)\n",
      "tensor(0.1254, grad_fn=<DivBackward0>)\n",
      "tensor(0.1237, grad_fn=<DivBackward0>)\n",
      "tensor(0.1319, grad_fn=<DivBackward0>)\n",
      "tensor(0.1212, grad_fn=<DivBackward0>)\n",
      "tensor(0.1209, grad_fn=<DivBackward0>)\n",
      "tensor(0.1096, grad_fn=<DivBackward0>)\n",
      "tensor(0.1097, grad_fn=<DivBackward0>)\n",
      "tensor(0.1103, grad_fn=<DivBackward0>)\n",
      "tensor(0.1135, grad_fn=<DivBackward0>)\n",
      "tensor(0.1102, grad_fn=<DivBackward0>)\n",
      "tensor(0.1041, grad_fn=<DivBackward0>)\n",
      "tensor(0.0950, grad_fn=<DivBackward0>)\n",
      "tensor(0.1040, grad_fn=<DivBackward0>)\n",
      "tensor(0.0969, grad_fn=<DivBackward0>)\n",
      "tensor(0.1009, grad_fn=<DivBackward0>)\n",
      "tensor(0.0968, grad_fn=<DivBackward0>)\n",
      "tensor(0.0993, grad_fn=<DivBackward0>)\n",
      "tensor(0.1006, grad_fn=<DivBackward0>)\n",
      "tensor(0.0915, grad_fn=<DivBackward0>)\n",
      "tensor(0.0919, grad_fn=<DivBackward0>)\n",
      "tensor(0.0885, grad_fn=<DivBackward0>)\n",
      "tensor(0.0898, grad_fn=<DivBackward0>)\n",
      "tensor(0.0881, grad_fn=<DivBackward0>)\n",
      "tensor(0.0830, grad_fn=<DivBackward0>)\n",
      "tensor(0.0821, grad_fn=<DivBackward0>)\n",
      "tensor(0.0810, grad_fn=<DivBackward0>)\n",
      "tensor(0.0822, grad_fn=<DivBackward0>)\n",
      "tensor(0.0821, grad_fn=<DivBackward0>)\n",
      "tensor(0.0847, grad_fn=<DivBackward0>)\n",
      "tensor(0.0796, grad_fn=<DivBackward0>)\n",
      "tensor(0.0756, grad_fn=<DivBackward0>)\n",
      "tensor(0.0760, grad_fn=<DivBackward0>)\n",
      "tensor(0.0760, grad_fn=<DivBackward0>)\n",
      "tensor(0.0747, grad_fn=<DivBackward0>)\n",
      "tensor(0.0796, grad_fn=<DivBackward0>)\n",
      "tensor(0.0733, grad_fn=<DivBackward0>)\n",
      "tensor(0.0720, grad_fn=<DivBackward0>)\n",
      "tensor(0.0725, grad_fn=<DivBackward0>)\n",
      "tensor(0.0672, grad_fn=<DivBackward0>)\n",
      "tensor(0.0697, grad_fn=<DivBackward0>)\n",
      "tensor(0.0682, grad_fn=<DivBackward0>)\n",
      "tensor(0.0707, grad_fn=<DivBackward0>)\n",
      "tensor(0.0654, grad_fn=<DivBackward0>)\n",
      "tensor(0.0632, grad_fn=<DivBackward0>)\n",
      "tensor(0.0658, grad_fn=<DivBackward0>)\n",
      "tensor(0.0628, grad_fn=<DivBackward0>)\n",
      "tensor(0.0654, grad_fn=<DivBackward0>)\n",
      "tensor(0.0623, grad_fn=<DivBackward0>)\n",
      "tensor(0.0606, grad_fn=<DivBackward0>)\n",
      "tensor(0.0626, grad_fn=<DivBackward0>)\n",
      "tensor(0.0604, grad_fn=<DivBackward0>)\n",
      "tensor(0.0625, grad_fn=<DivBackward0>)\n",
      "tensor(0.0599, grad_fn=<DivBackward0>)\n",
      "tensor(0.0586, grad_fn=<DivBackward0>)\n",
      "tensor(0.0575, grad_fn=<DivBackward0>)\n",
      "tensor(0.0545, grad_fn=<DivBackward0>)\n",
      "tensor(0.0557, grad_fn=<DivBackward0>)\n",
      "tensor(0.0550, grad_fn=<DivBackward0>)\n",
      "tensor(0.0533, grad_fn=<DivBackward0>)\n",
      "tensor(0.0526, grad_fn=<DivBackward0>)\n",
      "tensor(0.0520, grad_fn=<DivBackward0>)\n",
      "tensor(0.0513, grad_fn=<DivBackward0>)\n",
      "tensor(0.0504, grad_fn=<DivBackward0>)\n",
      "tensor(0.0498, grad_fn=<DivBackward0>)\n",
      "tensor(0.0468, grad_fn=<DivBackward0>)\n",
      "tensor(0.0484, grad_fn=<DivBackward0>)\n",
      "tensor(0.0485, grad_fn=<DivBackward0>)\n",
      "tensor(0.0452, grad_fn=<DivBackward0>)\n",
      "tensor(0.0479, grad_fn=<DivBackward0>)\n",
      "tensor(0.0471, grad_fn=<DivBackward0>)\n",
      "tensor(0.0467, grad_fn=<DivBackward0>)\n",
      "tensor(0.0430, grad_fn=<DivBackward0>)\n",
      "tensor(0.0428, grad_fn=<DivBackward0>)\n",
      "tensor(0.0440, grad_fn=<DivBackward0>)\n",
      "tensor(0.0450, grad_fn=<DivBackward0>)\n",
      "tensor(0.0418, grad_fn=<DivBackward0>)\n",
      "tensor(0.0409, grad_fn=<DivBackward0>)\n",
      "tensor(0.0439, grad_fn=<DivBackward0>)\n",
      "tensor(0.0407, grad_fn=<DivBackward0>)\n",
      "tensor(0.0408, grad_fn=<DivBackward0>)\n",
      "tensor(0.0386, grad_fn=<DivBackward0>)\n",
      "tensor(0.0400, grad_fn=<DivBackward0>)\n",
      "tensor(0.0385, grad_fn=<DivBackward0>)\n",
      "tensor(0.0407, grad_fn=<DivBackward0>)\n",
      "tensor(0.0383, grad_fn=<DivBackward0>)\n",
      "tensor(0.0368, grad_fn=<DivBackward0>)\n",
      "tensor(0.0380, grad_fn=<DivBackward0>)\n",
      "tensor(0.0359, grad_fn=<DivBackward0>)\n",
      "tensor(0.0355, grad_fn=<DivBackward0>)\n",
      "tensor(0.0359, grad_fn=<DivBackward0>)\n",
      "tensor(0.0345, grad_fn=<DivBackward0>)\n",
      "tensor(0.0343, grad_fn=<DivBackward0>)\n",
      "tensor(0.0340, grad_fn=<DivBackward0>)\n",
      "tensor(0.0342, grad_fn=<DivBackward0>)\n",
      "tensor(0.0333, grad_fn=<DivBackward0>)\n",
      "tensor(0.0328, grad_fn=<DivBackward0>)\n",
      "tensor(0.0331, grad_fn=<DivBackward0>)\n",
      "tensor(0.0323, grad_fn=<DivBackward0>)\n",
      "tensor(0.0320, grad_fn=<DivBackward0>)\n",
      "tensor(0.0320, grad_fn=<DivBackward0>)\n",
      "tensor(0.0306, grad_fn=<DivBackward0>)\n",
      "tensor(0.0320, grad_fn=<DivBackward0>)\n",
      "tensor(0.0302, grad_fn=<DivBackward0>)\n",
      "tensor(0.0302, grad_fn=<DivBackward0>)\n",
      "tensor(0.0301, grad_fn=<DivBackward0>)\n",
      "tensor(0.0292, grad_fn=<DivBackward0>)\n",
      "tensor(0.0283, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    cnt = 0\n",
    "    l = 0\n",
    "    for data in trainset:\n",
    "        data = data[0].squeeze()\n",
    "        x = torch.reshape(data, (batch_size, 784))\n",
    "        y = x.clone()\n",
    "        optimizer.zero_grad()\n",
    "        y_hat, h = ae(x)\n",
    "        loss = ae.loss(x, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        l += loss\n",
    "        cnt += 1\n",
    "        if cnt % 600 == 0:\n",
    "            cnt = 0\n",
    "            print(l / 600)\n",
    "            l = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "cnt = 0\n",
    "os.makedirs(\"img\", exist_ok=True)\n",
    "for test in testset:\n",
    "    data = test[0].squeeze()\n",
    "    x = torch.reshape(data, (batch_size, 784))\n",
    "    out = ae(x)[0].detach().numpy()\n",
    "    cnt += 1\n",
    "    for i in range(data.shape[0]):\n",
    "        plt.imsave('./img/' + str(cnt) + str(i) + 'org.png', data[i], cmap='gray')\n",
    "        plt.imsave('./img/' + str(cnt) + str(i) + 'recovered.png', out[i].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = list(ae.parameters())\n",
    "en = w[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"latent\", exist_ok=True)\n",
    "cnt = 0\n",
    "for i in en:\n",
    "    cnt += 1\n",
    "    plt.imsave('./latent/' + str(cnt)+ 'latent.png', i.reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import cv2\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from ipywidgets import widgets, Layout\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "from maskrcnn_benchmark.config import cfg\n",
    "from maskrcnn_benchmark.layers import nms\n",
    "from maskrcnn_benchmark.modeling.detector import build_detection_model\n",
    "from maskrcnn_benchmark.structures.image_list import to_image_list\n",
    "from maskrcnn_benchmark.utils.model_serialization import load_state_dict\n",
    "\n",
    "\n",
    "class FeatureExtractor:\n",
    "  TARGET_IMAGE_SIZE = [448, 448]\n",
    "  CHANNEL_MEAN = [0.485, 0.456, 0.406]\n",
    "  CHANNEL_STD = [0.229, 0.224, 0.225]\n",
    "  def __init__(self):\n",
    "    # self._init_processors()\n",
    "    self.detection_model = self._build_detection_model()\n",
    "  \n",
    "  def __call__(self, url):\n",
    "    with torch.no_grad():\n",
    "      detectron_features = self.get_detectron_features(url)\n",
    "    \n",
    "    return detectron_features\n",
    "  \n",
    "  def _build_detection_model(self):\n",
    "\n",
    "      cfg.merge_from_file('/Users/zetong/image_captioning/content/model_data/detectron_config.yaml')\n",
    "      cfg.freeze()\n",
    "\n",
    "      model = build_detection_model(cfg)\n",
    "      checkpoint = torch.load('/Users/zetong/image_captioning/content/model_data/detectron_model.pth', \n",
    "                              map_location=torch.device(\"cpu\"))\n",
    "\n",
    "      load_state_dict(model, checkpoint.pop(\"model\"))\n",
    "\n",
    "      model.eval()\n",
    "      return model\n",
    "  \n",
    "  def get_actual_image(self, image_path):\n",
    "      if image_path.startswith('http'):\n",
    "          path = requests.get(image_path, stream=True).raw\n",
    "      else:\n",
    "          path = image_path\n",
    "      \n",
    "      return path\n",
    "\n",
    "  def _image_transform(self, image_path):\n",
    "      path = self.get_actual_image(image_path)\n",
    "\n",
    "      img = Image.open(path)\n",
    "      im = np.array(img).astype(np.float32)\n",
    "      im = im[:, :, ::-1]\n",
    "      im -= np.array([102.9801, 115.9465, 122.7717])\n",
    "      im_shape = im.shape\n",
    "      im_size_min = np.min(im_shape[0:2])\n",
    "      im_size_max = np.max(im_shape[0:2])\n",
    "      im_scale = float(800) / float(im_size_min)\n",
    "      # Prevent the biggest axis from being more than max_size\n",
    "      if np.round(im_scale * im_size_max) > 1333:\n",
    "           im_scale = float(1333) / float(im_size_max)\n",
    "      im = cv2.resize(\n",
    "           im,\n",
    "           None,\n",
    "           None,\n",
    "           fx=im_scale,\n",
    "           fy=im_scale,\n",
    "           interpolation=cv2.INTER_LINEAR\n",
    "       )\n",
    "      img = torch.from_numpy(im).permute(2, 0, 1)\n",
    "      return img, im_scale\n",
    "\n",
    "\n",
    "  def _process_feature_extraction(self, output,\n",
    "                                 im_scales,\n",
    "                                 feat_name='fc6',\n",
    "                                 conf_thresh=0.2):\n",
    "      batch_size = len(output[0][\"proposals\"])\n",
    "      n_boxes_per_image = [len(_) for _ in output[0][\"proposals\"]]\n",
    "      score_list = output[0][\"scores\"].split(n_boxes_per_image)\n",
    "      score_list = [torch.nn.functional.softmax(x, -1) for x in score_list]\n",
    "      feats = output[0][feat_name].split(n_boxes_per_image)\n",
    "      cur_device = score_list[0].device\n",
    "\n",
    "      feat_list = []\n",
    "\n",
    "      for i in range(batch_size):\n",
    "          dets = output[0][\"proposals\"][i].bbox / im_scales[i]\n",
    "          scores = score_list[i]\n",
    "\n",
    "          max_conf = torch.zeros((scores.shape[0])).to(cur_device)\n",
    "\n",
    "          for cls_ind in range(1, scores.shape[1]):\n",
    "              cls_scores = scores[:, cls_ind]\n",
    "              keep = nms(dets, cls_scores, 0.5)\n",
    "              max_conf[keep] = torch.where(cls_scores[keep] > max_conf[keep],\n",
    "                                           cls_scores[keep],\n",
    "                                           max_conf[keep])\n",
    "\n",
    "          keep_boxes = torch.argsort(max_conf, descending=True)[:100]\n",
    "          feat_list.append(feats[i][keep_boxes])\n",
    "      return feat_list\n",
    "    \n",
    "  def get_detectron_features(self, image_path):\n",
    "      im, im_scale = self._image_transform(image_path)\n",
    "      img_tensor, im_scales = [im], [im_scale]\n",
    "      current_img_list = to_image_list(img_tensor, size_divisible=32)\n",
    "      #current_img_list = current_img_list.to('cuda')\n",
    "      with torch.no_grad():\n",
    "          output = self.detection_model(current_img_list)\n",
    "      feat_list = self._process_feature_extraction(output, im_scales, \n",
    "                                                  'fc6', 0.2)\n",
    "      return feat_list[0]\n",
    "\n",
    "feature_extractor = FeatureExtractor()\n",
    "\n",
    "import captioning\n",
    "import captioning.utils.misc\n",
    "import captioning.models\n",
    "infos = captioning.utils.misc.pickle_load(open('infos_trans12-best.pkl', 'rb'))\n",
    "infos['opt'].vocab = infos['vocab']\n",
    "model = captioning.models.setup(infos['opt'])\n",
    "model.load_state_dict(torch.load('model-best.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "image_path = feature_extractor.get_actual_image(\"/Users/zetong/Desktop/56ED4D4057E2D074E89B0B1DF6ABE4A1.jpg\")\n",
    "image = Image.open(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/zetong/Desktop/56ED4D4057E2D074E89B0B1DF6ABE4A1.jpg'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a desk with a computer and a chair',\n",
       " 'a desk with a computer on top of it',\n",
       " 'a desk with a computer monitor and a chair',\n",
       " 'a home office with a desk chair and computer',\n",
       " 'a home office with a desk chair and computer desk']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_captions(feature_extractor(\"/Users/zetong/Desktop/4.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"/Users/zetong/frames/\"\n",
    "files = []\n",
    "for filename in os.listdir(img_dir):\n",
    "    if \".jpg\" in filename:\n",
    "        files.append(img_dir + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/zetong/frames/300.jpg',\n",
       " '/Users/zetong/frames/600.jpg',\n",
       " '/Users/zetong/frames/900.jpg',\n",
       " '/Users/zetong/frames/1200.jpg',\n",
       " '/Users/zetong/frames/1500.jpg',\n",
       " '/Users/zetong/frames/1800.jpg',\n",
       " '/Users/zetong/frames/2100.jpg',\n",
       " '/Users/zetong/frames/2400.jpg',\n",
       " '/Users/zetong/frames/2700.jpg',\n",
       " '/Users/zetong/frames/3000.jpg',\n",
       " '/Users/zetong/frames/3300.jpg',\n",
       " '/Users/zetong/frames/3600.jpg',\n",
       " '/Users/zetong/frames/3900.jpg',\n",
       " '/Users/zetong/frames/4200.jpg',\n",
       " '/Users/zetong/frames/4500.jpg',\n",
       " '/Users/zetong/frames/4800.jpg',\n",
       " '/Users/zetong/frames/5100.jpg',\n",
       " '/Users/zetong/frames/5400.jpg',\n",
       " '/Users/zetong/frames/5700.jpg',\n",
       " '/Users/zetong/frames/6000.jpg',\n",
       " '/Users/zetong/frames/6300.jpg']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files.sort(key=lambda x: int(x[len(img_dir):-4]))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = []\n",
    "for f in files:\n",
    "    descriptions.append(get_captions(feature_extractor(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = [d[0] for d in descriptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a man taking a selfie in front of a store window',\n",
       " 'two men sitting in front of a computer screen',\n",
       " 'a man wearing glasses and a black shirt',\n",
       " 'a man wearing glasses and a tie',\n",
       " 'a couple of people sitting next to a window',\n",
       " 'a group of people standing in a room',\n",
       " 'a man and a woman standing in front of a door',\n",
       " 'a group of people in a large room',\n",
       " 'a man sitting in the back of a pick up truck',\n",
       " 'a group of men standing next to each other',\n",
       " 'a woman sitting next to a man in a car',\n",
       " 'a person driving a car on a city street',\n",
       " 'a man is talking on a cell phone',\n",
       " 'a couple of cars driving across a desert field',\n",
       " 'a group of cars driving down a street',\n",
       " 'a man wearing a pair of sunglasses and a helmet',\n",
       " 'a man in a plaid shirt and sunglasses',\n",
       " 'a group of cars driving down a road',\n",
       " 'a couple of cars parked on top of a sandy beach',\n",
       " 'a person driving a car down a road',\n",
       " 'a woman standing next to a bed in a room']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

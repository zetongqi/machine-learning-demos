{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def sigmoid(z):\n",
    "    a = 1/(1+np.exp(-z))\n",
    "    return a\n",
    "\n",
    "def relu(z):\n",
    "    a = np.maximum(0, z)\n",
    "    return a\n",
    "\n",
    "def leaky_relu(z, aplha=0.01):\n",
    "    a = np.maximum(aplha*z, z)\n",
    "    return a\n",
    "\n",
    "def tanh(z):\n",
    "    a = (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "    return a\n",
    "\n",
    "def softmax(z):\n",
    "    m = z.shape[-1]\n",
    "    exp_z = np.exp(z)\n",
    "    a = np.divide(exp_z, np.sum(exp_z)/m)\n",
    "    return a\n",
    "\n",
    "def activate(z, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        return sigmoid(z)\n",
    "    elif activation == \"relu\":\n",
    "        return relu(z)\n",
    "    elif activation == \"leaky_relu\":\n",
    "        return leaky_relu(z)\n",
    "    elif activation == \"tanh\":\n",
    "        return tanh(z)\n",
    "    else:\n",
    "        return softmax(z)\n",
    "\n",
    "class layer:\n",
    "    def __init__(self, shape, activation):\n",
    "        self.W = np.random.randn(*shape)*0.01\n",
    "        self.b = np.zeros((shape[0], 1))\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.z = np.dot(self.W, x) + self.b\n",
    "        self.a = activate(self.z, self.activation)\n",
    "        return self.a, self.W, self.z, x\n",
    "\n",
    "def backward_propagation_sigmoid(dal, wl, zl, al_1):\n",
    "    m = dal.shape[-1]\n",
    "    al = sigmoid(zl)\n",
    "    dzl = dal * al * (1 - al)\n",
    "    dwl = 1/m*np.dot(dzl, al_1.T)\n",
    "    dbl = 1/m*np.sum(dzl, axis=1, keepdims=True)\n",
    "    dal_1 = np.dot(wl.T, dzl)\n",
    "    return dal_1, dwl, dbl\n",
    "\n",
    "def backward_propagation_relu(dal, wl, zl, al_1):\n",
    "    m = dal.shape[-1]\n",
    "    al = relu(zl)\n",
    "    dal_dzl = zl >= 0\n",
    "    dal_dzl = dal_dzl.astype(\"int\")\n",
    "    dzl = dal*dal_dzl\n",
    "    dwl = 1/m*np.dot(dzl, al_1.T)\n",
    "    dbl = 1/m*np.sum(dzl, axis=1, keepdims=True)\n",
    "    dal_1 = np.dot(wl.T, dzl)\n",
    "    return dal_1, dwl, dbl\n",
    "\n",
    "def backward_propagation_tanh(dal, wl, zl, al_1):\n",
    "    m = dal.shape[-1]\n",
    "    al = tanh(zl)\n",
    "    dzl = dal*(1-al**2)\n",
    "    dwl = 1/m*np.dot(dzl, al_1.T)\n",
    "    dbl = 1/m*np.sum(dzl, axis=1, keepdims=True)\n",
    "    dal_1 = np.dot(wl.T, dzl)\n",
    "    return dal_1, dwl, dbl\n",
    "\n",
    "def backward_propagation(dal, wl, zl, al_1, act):\n",
    "    if act == \"sigmoid\":\n",
    "        return backward_propagation_sigmoid(dal, wl, zl, al_1)\n",
    "    elif act == \"relu\":\n",
    "        return backward_propagation_relu(dal, wl, zl, al_1)\n",
    "    else:\n",
    "        return backward_propagation_tanh(dal, wl, zl, al_1)\n",
    "\n",
    "def select(a, c):\n",
    "    ac = []\n",
    "    for i, j in zip(c[0, :], range(c.shape[-1])):\n",
    "        ac.append(a[i, j])\n",
    "    ac = np.array(ac).reshape(1, len(ac))\n",
    "    return ac\n",
    "\n",
    "def grads_correction(dal_dzl, temp, c):\n",
    "    for idx, i in zip(c[0, :], range(dal_dzl.shape[-1])):\n",
    "        dal_dzl[idx, i] = temp[0, i]\n",
    "    return dal_dzl\n",
    "\n",
    "def backward_propagation_softmax(dal, wl, zl, al_1, c):\n",
    "    m = dal.shape[-1]\n",
    "    al = softmax(zl)\n",
    "    ac = select(al, c)\n",
    "    dal_dzl = -al * ac\n",
    "    temp = ac*(1-ac)\n",
    "    dal_dzl = grads_correction(dal_dzl, temp, c)\n",
    "    dzl = dal*dal_dzl\n",
    "    dwl = 1/m*np.dot(dzl, al_1.T)\n",
    "    dbl = 1/m*np.sum(dzl, axis=1, keepdims=True)\n",
    "    dal_1 = np.dot(wl.T, dzl)\n",
    "    return dal_1, dwl, dbl\n",
    "\n",
    "def one_hot(num_classes, y):\n",
    "    zero_array = np.zeros((num_classes, y.shape[-1]))\n",
    "    for y_, i in zip(y[0, :], range(y.shape[-1])):\n",
    "        zero_array[y_, i] = 1\n",
    "    return zero_array\n",
    "\n",
    "class model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def forward_prop(self, x):\n",
    "        cache = {}\n",
    "        al = x\n",
    "        for layer, l in zip(self.layers, range(1, len(self.layers)+1)):\n",
    "            al, wl, zl, al_1 = layer.forward(al)\n",
    "            cache[\"l\"+str(l)] = [wl, zl, al_1]\n",
    "        return al, cache\n",
    "    \n",
    "    def backward_prop(self, y_hat, y, cache):\n",
    "        grads = {}\n",
    "        y_onehot = one_hot(y_hat.shape[0], y)\n",
    "        m = y.shape[-1]\n",
    "        dzl = np.copy(y_hat)\n",
    "        dzl[y, range(m)] -= 1\n",
    "        wl, zl, al_1 = cache[\"l\"+str(len(self.layers))]\n",
    "        dwl = 1/m*np.dot(dzl, al_1.T)\n",
    "        dbl = 1/m*np.sum(dzl, axis=1, keepdims=True)\n",
    "        dal = np.dot(wl.T, dzl)\n",
    "        grads[\"dW\"+str(len(self.layers))] = dwl\n",
    "        grads[\"db\"+str(len(self.layers))] = dbl\n",
    "        \n",
    "        for i in reversed(range(1, len(self.layers))):\n",
    "            wl, zl, al_1 = cache[\"l\"+str(i)]\n",
    "            dal, dwl, dbl = backward_propagation(dal, wl, zl, al_1, self.layers[i-1].activation)\n",
    "            grads[\"dW\"+str(i)] = dwl\n",
    "            grads[\"db\"+str(i)] = dbl\n",
    "        return grads\n",
    "    \n",
    "    def loss(self, y_hat, y):\n",
    "        m = y.shape[-1]\n",
    "        J = -1/m*np.sum(y*np.log(y_hat))\n",
    "        return J\n",
    "    \n",
    "    def update(self, grads, learning_rate):\n",
    "        for layer, i in zip(self.layers, range(1, len(self.layers)+1)):\n",
    "            layer.W = layer.W - learning_rate*grads[\"dW\"+str(i)]\n",
    "            layer.b = layer.b - learning_rate*grads[\"db\"+str(i)]\n",
    "    \n",
    "    def train(self, x, y, epoches, learning_rate):\n",
    "        for i in range(epoches):\n",
    "            y_onehot = one_hot(self.layers[-1].b.shape[0], y)\n",
    "            y_hat, cache = self.forward_prop(x)\n",
    "            J = self.loss(y_hat, y_onehot)\n",
    "            grads = self.backward_prop(y_hat, y, cache)\n",
    "            self.update(grads, learning_rate)\n",
    "            if i %(epoches/10) == 0:\n",
    "                print(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 150) (1, 150)\n"
     ]
    }
   ],
   "source": [
    "data = load_iris()\n",
    "x = data[\"data\"].T\n",
    "y = data[\"target\"].reshape(1, x.shape[-1])\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0983897016594966\n",
      "0.5104690154092868\n",
      "0.3665867548607246\n",
      "0.2849938743484492\n",
      "0.22756053704051768\n",
      "0.18175294286213525\n",
      "0.14987405914649674\n",
      "0.1303453709448769\n",
      "0.11982219268256375\n",
      "0.11431554127093613\n"
     ]
    }
   ],
   "source": [
    "nets = model()\n",
    "layers = [layer((6,4), \"tanh\"), layer((3,6), \"softmax\")]\n",
    "nets.layers = layers\n",
    "nets.train(x, y, 1000, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat, _ = nets.forward_prop(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_hat, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

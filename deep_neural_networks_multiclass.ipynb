{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def sigmoid(z):\n",
    "    a = 1/(1+np.exp(-z))\n",
    "    return a\n",
    "\n",
    "def relu(z):\n",
    "    a = np.maximum(0, z)\n",
    "    return a\n",
    "\n",
    "def leaky_relu(z, aplha=0.01):\n",
    "    a = np.maximum(aplha*z, z)\n",
    "    return a\n",
    "\n",
    "def tanh(z):\n",
    "    a = (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "    return a\n",
    "\n",
    "def softmax(z):\n",
    "    m = z.shape[-1]\n",
    "    exp_z = np.exp(z)\n",
    "    a = np.divide(exp_z, np.sum(exp_z)/m)\n",
    "    return a\n",
    "\n",
    "def activate(z, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        return sigmoid(z)\n",
    "    elif activation == \"relu\":\n",
    "        return relu(z)\n",
    "    elif activation == \"leaky_relu\":\n",
    "        return leaky_relu(z)\n",
    "    elif activation == \"tanh\":\n",
    "        return tanh(z)\n",
    "    else:\n",
    "        return softmax(z)\n",
    "\n",
    "class layer:\n",
    "    def __init__(self, shape, activation):\n",
    "        self.W = np.random.randn(*shape)*0.01\n",
    "        self.b = np.zeros((shape[0], 1))\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.z = np.dot(self.W, x) + self.b\n",
    "        self.a = activate(self.z, self.activation)\n",
    "        return self.a, self.W, self.z, x\n",
    "\n",
    "def backward_propagation_sigmoid(dal, wl, zl, al_1):\n",
    "    m = dal.shape[-1]\n",
    "    al = sigmoid(zl)\n",
    "    dzl = dal * al * (1 - al)\n",
    "    dwl = 1/m*np.dot(dzl, al_1.T)\n",
    "    dbl = 1/m*np.sum(dzl, axis=1, keepdims=True)\n",
    "    dal_1 = np.dot(wl.T, dzl)\n",
    "    return dal_1, dwl, dbl\n",
    "\n",
    "def backward_propagation_relu(dal, wl, zl, al_1):\n",
    "    m = dal.shape[-1]\n",
    "    al = relu(zl)\n",
    "    dal_dzl = zl >= 0\n",
    "    dal_dzl = dal_dzl.astype(\"int\")\n",
    "    dzl = dal*dal_dzl\n",
    "    dwl = 1/m*np.dot(dzl, al_1.T)\n",
    "    dbl = 1/m*np.sum(dzl, axis=1, keepdims=True)\n",
    "    dal_1 = np.dot(wl.T, dzl)\n",
    "    return dal_1, dwl, dbl\n",
    "\n",
    "def backward_propagation_tanh(dal, wl, zl, al_1):\n",
    "    m = dal.shape[-1]\n",
    "    al = tanh(zl)\n",
    "    dzl = dal*(1-al**2)\n",
    "    dwl = 1/m*np.dot(dzl, al_1.T)\n",
    "    dbl = 1/m*np.sum(dzl, axis=1, keepdims=True)\n",
    "    dal_1 = np.dot(wl.T, dzl)\n",
    "    return dal_1, dwl, dbl\n",
    "\n",
    "def backward_propagation(dal, wl, zl, al_1, act):\n",
    "    if act == \"sigmoid\":\n",
    "        return backward_propagation_sigmoid(dal, wl, zl, al_1)\n",
    "    elif act == \"relu\":\n",
    "        return backward_propagation_relu(dal, wl, zl, al_1)\n",
    "    else:\n",
    "        return backward_propagation_tanh(dal, wl, zl, al_1)\n",
    "\n",
    "def select(a, c):\n",
    "    ac = []\n",
    "    for i, j in zip(c[0, :], range(c.shape[-1])):\n",
    "        ac.append(a[i, j])\n",
    "    ac = np.array(ac).reshape(1, len(ac))\n",
    "    return ac\n",
    "\n",
    "def grads_correction(dal_dzl, temp, c):\n",
    "    for idx, i in zip(c[0, :], range(dal_dzl.shape[-1])):\n",
    "        dal_dzl[idx, i] = temp[0, i]\n",
    "    return dal_dzl\n",
    "\n",
    "def backward_propagation_softmax(dal, wl, zl, al_1, c):\n",
    "    m = dal.shape[-1]\n",
    "    al = softmax(zl)\n",
    "    ac = select(al, c)\n",
    "    dal_dzl = -al * ac\n",
    "    temp = ac*(1-ac)\n",
    "    dal_dzl = grads_correction(dal_dzl, temp, c)\n",
    "    dzl = dal*dal_dzl\n",
    "    dwl = 1/m*np.dot(dzl, al_1.T)\n",
    "    dbl = 1/m*np.sum(dzl, axis=1, keepdims=True)\n",
    "    dal_1 = np.dot(wl.T, dzl)\n",
    "    return dal_1, dwl, dbl\n",
    "\n",
    "def one_hot(num_classes, y):\n",
    "    zero_array = np.zeros((num_classes, y.shape[-1]))\n",
    "    for y_, i in zip(y[0, :], range(y.shape[-1])):\n",
    "        zero_array[y_, i] = 1\n",
    "    return zero_array\n",
    "\n",
    "# create batches of data\n",
    "def batchify(X, Y, batch_size):\n",
    "    m = X.shape[-1]\n",
    "    for idx in range(0, m, batch_size):\n",
    "        yield(X[:, idx:min(idx + batch_size, m)], Y[:, idx:min(idx + batch_size, m)])\n",
    "\n",
    "class model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def forward_prop(self, x):\n",
    "        cache = {}\n",
    "        al = x\n",
    "        for layer, l in zip(self.layers, range(1, len(self.layers)+1)):\n",
    "            al, wl, zl, al_1 = layer.forward(al)\n",
    "            cache[\"l\"+str(l)] = [wl, zl, al_1]\n",
    "        return al, cache\n",
    "    \n",
    "    def backward_prop(self, y_hat, y, cache):\n",
    "        grads = {}\n",
    "        y_onehot = one_hot(y_hat.shape[0], y)\n",
    "        m = y.shape[-1]\n",
    "        dzl = np.copy(y_hat)\n",
    "        dzl[y, range(m)] -= 1\n",
    "        wl, zl, al_1 = cache[\"l\"+str(len(self.layers))]\n",
    "        dwl = 1/m*np.dot(dzl, al_1.T)\n",
    "        dbl = 1/m*np.sum(dzl, axis=1, keepdims=True)\n",
    "        dal = np.dot(wl.T, dzl)\n",
    "        #print(dbl)\n",
    "        grads[\"dW\"+str(len(self.layers))] = dwl\n",
    "        grads[\"db\"+str(len(self.layers))] = dbl\n",
    "        \n",
    "        for i in reversed(range(1, len(self.layers))):\n",
    "            wl, zl, al_1 = cache[\"l\"+str(i)]\n",
    "            dal, dwl, dbl = backward_propagation(dal, wl, zl, al_1, self.layers[i-1].activation)\n",
    "            grads[\"dW\"+str(i)] = dwl\n",
    "            grads[\"db\"+str(i)] = dbl\n",
    "        return grads\n",
    "    \n",
    "    def loss(self, y_hat, y):\n",
    "        m = y.shape[-1]\n",
    "        J = -1/m*np.sum(y*np.log(y_hat))\n",
    "        return J\n",
    "    \n",
    "    def update(self, grads, learning_rate):\n",
    "        for layer, i in zip(self.layers, range(1, len(self.layers)+1)):\n",
    "            layer.W = layer.W - learning_rate*grads[\"dW\"+str(i)]\n",
    "            layer.b = layer.b - learning_rate*grads[\"db\"+str(i)]\n",
    "    \n",
    "    def train(self, x, y, epoches, learning_rate, batch_size):\n",
    "        losses = []\n",
    "        for i in range(epoches):\n",
    "            for X_batch, Y_batch in batchify(x, y, batch_size):\n",
    "                y_onehot = one_hot(self.layers[-1].b.shape[0], Y_batch)\n",
    "                y_hat, cache = self.forward_prop(X_batch)\n",
    "                J = self.loss(y_hat, y_onehot)\n",
    "                grads = self.backward_prop(y_hat, Y_batch, cache)\n",
    "                self.update(grads, learning_rate)\n",
    "            if i %(epoches/10) == 0:\n",
    "                print(J)\n",
    "            losses.append(J)\n",
    "        return losses\n",
    "    \n",
    "    def evaluate(self, x, y):\n",
    "        y_hat, _ = self.forward_prop(x)\n",
    "        acc = (np.argmax(y_hat, axis=0) == y).astype(\"int\")\n",
    "        acc = np.sum(acc) / acc.shape[-1]\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1257) (1, 1257)\n"
     ]
    }
   ],
   "source": [
    "data = load_digits()\n",
    "x = data[\"data\"].T\n",
    "y = data[\"target\"].reshape(1, x.shape[-1])\n",
    "\n",
    "def split(X, Y, ratio, rand_state=1):\n",
    "    X = X.T\n",
    "    Y = Y.T\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=ratio, random_state=rand_state)\n",
    "    X_train = X_train.T\n",
    "    X_test = X_test.T\n",
    "    Y_train = Y_train.T\n",
    "    Y_test = Y_test.T\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = split(x, y, 0.3)\n",
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.084320072069936\n",
      "0.001521597523028939\n",
      "0.0006474677725406452\n",
      "0.0004427932244254944\n",
      "0.0003078609367274892\n",
      "0.00023764599844507416\n",
      "0.00021117331020050736\n",
      "0.00016076341149558933\n",
      "0.00015207183970701065\n",
      "0.00014691034560256698\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "nets = model()\n",
    "layers = [layer((128, 64), \"sigmoid\"), layer((10,128), \"softmax\")]\n",
    "nets.layers = layers\n",
    "losses = nets.train(X_train, Y_train, 10000, 0.1, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 98.51851851851852%\n"
     ]
    }
   ],
   "source": [
    "acc = nets.evaluate(X_test, Y_test)\n",
    "print(\"test accuracy: \"+str(acc*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
